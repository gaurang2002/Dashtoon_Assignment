{"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":6988774,"sourceType":"datasetVersion","datasetId":4016740}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\" Downloading Dataset\"\"\"\n!wget http://images.cocodataset.org/zips/val2017.zip\n!mkdir './Dataset_new'\n!unzip -q ./val2017.zip -d './Dataset_new'","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6oOtOOTYhMnQ","outputId":"1f3ed172-033c-4da6-c85d-eaeec4ffceff"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"--2023-11-18 14:53:03--  http://images.cocodataset.org/zips/val2017.zip\n\nResolving images.cocodataset.org (images.cocodataset.org)... 52.217.114.241, 52.216.51.89, 52.217.122.17, ...\n\nConnecting to images.cocodataset.org (images.cocodataset.org)|52.217.114.241|:80... connected.\n\nHTTP request sent, awaiting response... 200 OK\n\nLength: 815585330 (778M) [application/zip]\n\nSaving to: ‘val2017.zip’\n\n\n\nval2017.zip         100%[===================>] 777.80M  18.0MB/s    in 50s     \n\n\n\n2023-11-18 14:53:54 (15.7 MB/s) - ‘val2017.zip’ saved [815585330/815585330]\n\n\n"}]},{"cell_type":"code","source":"\"\"\" Downloading content and style images \"\"\"\n!mkdir ./content\n!mkdir ./style\n!wget -q https://github.com/myelinfoundry-2019/challenge/raw/master/japanese_garden.jpg -P './content'\n!wget -q https://github.com/myelinfoundry-2019/challenge/raw/master/picasso_selfportrait.jpg -P './style'","metadata":{"id":"ekFa6Z-xzMZs"},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nimport torch\nfrom torch.autograd import Variable\nfrom collections import namedtuple\nfrom torchvision import models\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nimport numpy as np\nimport os\nimport shutil\nfrom sklearn.model_selection import train_test_split\nimport sys\nimport random\nfrom PIL import Image\nimport glob\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom skimage.metrics import structural_similarity as ssim\nfrom torchvision import datasets\nfrom torchvision.utils import save_image\nimport matplotlib.pyplot as plt\nimport cv2\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(42) #for reproducibility\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Mean and standard deviation used for training\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])","metadata":{"id":"Nrt4hIC3uy3O"},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Dividing Dataset in train,validation and test\ndataset_root = '/content/Dataset_new/val2017'\noutput_root = '/content/dataset_new'\n\ntrain_dir = os.path.join(output_root, 'train')\ntest_dir = os.path.join(output_root, 'test')\nvalidation_dir = os.path.join(output_root, 'validation')\n\nos.makedirs(train_dir, exist_ok=True)\nos.makedirs(test_dir, exist_ok=True)\nos.makedirs(validation_dir, exist_ok=True)\n\ntrain_dir = os.path.join(train_dir, 'train')\ntest_dir = os.path.join(test_dir, 'test')\nvalidation_dir = os.path.join(validation_dir, 'validation')\n\nos.makedirs(train_dir, exist_ok=True)\nos.makedirs(test_dir, exist_ok=True)\nos.makedirs(validation_dir, exist_ok=True)\n\nall_files = os.listdir(dataset_root)\n\ntrain_files, test_validation_files = train_test_split(all_files, test_size=0.2, random_state=42)\ntest_files, validation_files = train_test_split(test_validation_files, test_size=0.5, random_state=42)\n\nfor file in train_files:\n    shutil.copy(os.path.join(dataset_root, file), os.path.join(train_dir, file))\n\nfor file in test_files:\n    shutil.copy(os.path.join(dataset_root, file), os.path.join(test_dir, file))\n\nfor file in validation_files:\n    shutil.copy(os.path.join(dataset_root, file), os.path.join(validation_dir, file))\n","metadata":{"id":"V9Frlbt-Ut-L"},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Models Used\nWe have chosen below 2 models\n1. **VGG16**: Pre-trained model for feature extraction for loss comparisions.\n2. **TransformerNet**: The main model which acts as an encoder-decoder pair and learns to convert any image to a specific style.","metadata":{"id":"ZwRPQ8XDu4Ls"}},{"cell_type":"code","source":"\"\"\" Pretrained VGG16 Model \"\"\"\nclass VGG16(torch.nn.Module):\n    def __init__(self, requires_grad=False):\n        super(VGG16, self).__init__()\n        vgg_pretrained_features = models.vgg16(pretrained=True).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n\n\n        for x in range(4):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(4, 9):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(9, 16):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(16, 23):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h = self.slice1(X)\n        h_relu1_2 = h\n        h = self.slice2(h)\n        h_relu2_2 = h\n        h = self.slice3(h)\n        h_relu3_3 = h\n        h = self.slice4(h)\n        h_relu4_3 = h\n        vgg_outputs = namedtuple(\"VggOutputs\", [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\"])\n        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3)\n        return out\n\n\n\"\"\" Transformer Net \"\"\"\nclass TransformerNet(torch.nn.Module):\n    def __init__(self):\n        super(TransformerNet, self).__init__()\n        self.model = nn.Sequential(\n            ConvBlock(3, 32, kernel_size=9, stride=1),\n            ConvBlock(32, 64, kernel_size=3, stride=2),\n            ConvBlock(64, 128, kernel_size=3, stride=2),\n            ResidualBlock(128),\n            ResidualBlock(128),\n            ResidualBlock(128),\n            ResidualBlock(128),\n            ResidualBlock(128),\n            ConvBlock(128, 64, kernel_size=3, upsample=True),\n            ConvBlock(64, 32, kernel_size=3, upsample=True),\n            ConvBlock(32, 3, kernel_size=9, stride=1, normalize=False, relu=False),\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n\n\"\"\" Components of Transformer Net \"\"\"\nclass ResidualBlock(torch.nn.Module):\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.block = nn.Sequential(\n            ConvBlock(channels, channels, kernel_size=3, stride=1, normalize=True, relu=True),\n            ConvBlock(channels, channels, kernel_size=3, stride=1, normalize=True, relu=False),\n        )\n\n    def forward(self, x):\n        return self.block(x) + x\n\n\nclass ConvBlock(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, upsample=False, normalize=True, relu=True):\n        super(ConvBlock, self).__init__()\n        self.upsample = upsample\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(kernel_size // 2), nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n        )\n        self.norm = nn.InstanceNorm2d(out_channels, affine=True) if normalize else None\n        self.relu = relu\n\n    def forward(self, x):\n        if self.upsample:\n            x = F.interpolate(x, scale_factor=2)\n        x = self.block(x)\n        if self.norm is not None:\n            x = self.norm(x)\n        if self.relu:\n            x = F.relu(x)\n        return x","metadata":{"id":"YlyzsaqPu1kl"},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\"\"\" Utility Functions \"\"\"\ndef gram_matrix(y):\n    (b, c, h, w) = y.size()\n    features = y.view(b, c, w * h)\n    features_t = features.transpose(1, 2)\n    gram = features.bmm(features_t) / (c * h * w)\n    return gram\n\ndef train_transform(image_size):\n    transform = transforms.Compose(\n        [\n            transforms.Resize((int(image_size * 1.15),int(image_size * 1.15))),\n            transforms.RandomCrop(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std),\n        ]\n    )\n    return transform\n\ndef validation_transform(image_size):\n    transform = transforms.Compose(\n        [\n            transforms.Resize((int(image_size * 1.15),int(image_size * 1.15))),\n            transforms.RandomCrop(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std),\n        ]\n    )\n    return transform\n\ndef style_transform(image_size=None):\n    resize = [transforms.Resize((image_size,image_size))] if image_size else []\n    transform = transforms.Compose(resize + [transforms.ToTensor(), transforms.Normalize(mean, std)])\n    return transform\n\ndef test_transform(image_size=None):\n    resize = [transforms.Resize(image_size)] if image_size else []\n    transform = transforms.Compose(resize + [transforms.ToTensor(), transforms.Normalize(mean, std)])\n    return transform\n\n# Denormalizes image tensors using mean and std\ndef denormalize(tensors):\n    for c in range(3):\n        tensors[:, c].mul_(std[c]).add_(mean[c])\n    return tensors\n\n# Denormalizes and rescales image tensor\ndef deprocess(image_tensor):\n    image_tensor = denormalize(image_tensor)[0]\n    image_tensor *= 255\n    image_np = torch.clamp(image_tensor, 0, 255).cpu().numpy().astype(np.uint8)\n    image_np = image_np.transpose(1, 2, 0)\n    return image_np","metadata":{"id":"tqnLDfJ5u7YY"},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"pip install torchmetrics","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kwexmHfMga3M","outputId":"3ceaf870-f86d-48ef-febd-ee9e4743bac1"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":"Collecting torchmetrics\n\n  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n\nRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu118)\n\nCollecting lightning-utilities>=0.8.0 (from torchmetrics)\n\n  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n\nRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.2)\n\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.13.1)\n\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.2.1)\n\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n\nRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n\nInstalling collected packages: lightning-utilities, torchmetrics\n\nSuccessfully installed lightning-utilities-0.10.0 torchmetrics-1.2.0\n"}]},{"cell_type":"code","source":"from torchmetrics.image import StructuralSimilarityIndexMeasure\ndef fast_trainer(style_image,\n                 style_name,\n                 train_dataset_path,\n                 validation_dataset_path,\n                 image_size=256,\n                 style_size=448,\n                 batch_size = 8,\n                 lr = 1e-5,\n                 epochs = 1,\n                 checkpoint_model = None,\n                 checkpoint_interval=200,\n                 sample_interval=200,\n                 lambda_style=10e10,\n                 lambda_content=10e5,):\n\n    os.makedirs(f\"./checkpoints\", exist_ok=True)\n\n    train_dataset = datasets.ImageFolder(train_dataset_path, train_transform(image_size))\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n\n    validation_dataset = datasets.ImageFolder(validation_dataset_path, validation_transform(image_size))\n    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size)\n\n    transformer = TransformerNet().to(device)\n    vgg = VGG16(requires_grad=False).to(device)\n\n    if checkpoint_model:\n        transformer.load_state_dict(torch.load(checkpoint_model))\n\n    optimizer = Adam(transformer.parameters(), lr)\n    l2_loss = torch.nn.MSELoss().to(device)\n\n    style = style_transform(style_size)(Image.open(style_image))\n    style = style.repeat(batch_size, 1, 1, 1).to(device)\n\n    features_style = vgg(style)\n    gram_style = [gram_matrix(y) for y in features_style]\n\n    ssim = StructuralSimilarityIndexMeasure().to(device)\n    train_metrics = {\"content\": [], \"style\": [], \"total\": [],\"ssim\": []}\n    val_metrics = {\"content\": [], \"style\": [], \"total\": [],\"ssim\": []}\n    for epoch in range(epochs):\n        val_ssim = []\n        train_ssim = []\n        epoch_metrics = {\"content\": [], \"style\": [], \"total\": [],\"ssim\": []}\n        for batch_i, (images, _) in enumerate(train_dataloader):\n            optimizer.zero_grad()\n\n            images_original = images.to(device)\n            images_transformed = transformer(images_original)\n\n            train_r  = ssim(images_original,images_transformed)\n            train_ssim.append(train_r)\n\n            features_original = vgg(images_original)\n            features_transformed = vgg(images_transformed)\n\n            content_loss = lambda_content * l2_loss(features_transformed.relu2_2, features_original.relu2_2)\n\n            style_loss = 0\n            for ft_y, gm_s in zip(features_transformed, gram_style):\n                gm_y = gram_matrix(ft_y)\n                style_loss += l2_loss(gm_y, gm_s[: images.size(0), :, :])\n            style_loss *= lambda_style\n\n            total_loss = content_loss + style_loss\n            total_loss.backward()\n            optimizer.step()\n            epoch_metrics[\"content\"] += [content_loss.item()]\n            epoch_metrics[\"style\"] += [style_loss.item()]\n            epoch_metrics[\"total\"] += [total_loss.item()]\n            epoch_metrics[\"ssim\"] += [train_r.item()]\n\n            train_metrics[\"content\"] += [content_loss.item()]\n            train_metrics[\"style\"] += [style_loss.item()]\n            train_metrics[\"total\"] += [total_loss.item()]\n            train_metrics[\"ssim\"] += [train_r.item()]\n\n            sys.stdout.write(\n                \"\\r[Epoch %d/%d] [Batch %d/%d] [ssim: %.2f] [Content: %.2f (%.2f) Style: %.2f (%.2f) Total: %.2f (%.2f)]\"\n                % (\n                    epoch + 1, epochs, batch_i,len(train_dataset),train_r, content_loss.item(),np.mean(epoch_metrics[\"content\"]),\n                    style_loss.item(), np.mean(epoch_metrics[\"style\"]), total_loss.item(), np.mean(epoch_metrics[\"total\"]),\n                )\n            )\n            batches_done = epoch * len(train_dataloader) + batch_i + 1\n            torch.save(transformer.state_dict(), f\"./checkpoints/last_checkpoint.pth\")\n\n        for batch_i, (images, _) in enumerate(validation_dataloader):\n            images_original = images.to(device)\n            images_transformed = transformer(images_original)\n            val_r  = ssim(images_original,images_transformed)\n            val_ssim.append(val_r)\n\n            features_original = vgg(images_original)\n            features_transformed = vgg(images_transformed)\n\n            content_loss_1 = lambda_content * l2_loss(features_transformed.relu2_2, features_original.relu2_2)\n\n            style_loss_1 = 0\n            for ft_y, gm_s in zip(features_transformed, gram_style):\n                gm_y = gram_matrix(ft_y)\n                style_loss_1 += l2_loss(gm_y, gm_s[: images.size(0), :, :])\n            style_loss_1 *= lambda_style\n\n            total_val_loss = content_loss_1 + style_loss_1\n\n            val_metrics[\"content\"] += [content_loss.item()]\n            val_metrics[\"style\"] += [style_loss.item()]\n            val_metrics[\"total\"] += [total_loss.item()]\n            val_metrics[\"ssim\"] += [val_r.item()]\n            sys.stdout.write(\n                \"\\r[Validation] [ssim: %.2f] [Content: %.2f (%.2f) Style: %.2f (%.2f) Total: %.2f (%.2f)]\"\n                % (\n                    val_r, content_loss.item(),np.mean(val_metrics[\"content\"]),\n                    style_loss.item(), np.mean(val_metrics[\"style\"]), total_loss.item(), np.mean(val_metrics[\"total\"]),\n                )\n            )\n\n    print(\"Training Completed!\")\n    plt.plot(val_metrics[\"content\"], label = \"Content Loss\")\n    plt.plot(val_metrics[\"style\"], label = \"Style Loss\")\n    plt.plot(val_metrics[\"total\"], label = \"Total Loss\")\n    plt.xlabel('Iteration')\n    plt.ylabel('Loss')\n    plt.title('Validation Loss')\n    plt.legend()\n    plt.show()","metadata":{"id":"F1NG_ZHqvVFU"},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\"\"\" Run this to train the model \"\"\"\nfast_trainer(style_image='/content/style/picasso_selfportrait.jpg',style_name = 'Picasso_Selfportrait',\n             train_dataset_path='/content/dataset_new/train', validation_dataset_path='/content/dataset_new/validation',epochs = 1)","metadata":{"id":"KPMRapsjqA5z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_check(test_dataset_path,checkpoint_model,image_size):\n    all_files = os.listdir(test_dataset_path)\n    transform = test_transform()\n    transformer = TransformerNet().to(device)\n    transformer.load_state_dict(torch.load(checkpoint_model))\n    transformer.eval()\n    ssim = StructuralSimilarityIndexMeasure().to(device)\n    total_test = []\n    for file in all_files:\n        image_path = os.path.join(test_dataset_path,file)\n        image_tensor = Variable(transform(Image.open(image_path))).to(device)\n        image_tensor = image_tensor.unsqueeze(0)\n        stylized_image = transformer(image_tensor)\n        if image_tensor.size() == stylized_image.size():\n            test_r = ssim(image_tensor,stylized_image)\n            total_test.append(test_r.int())\n\n    print(np.mean(total_test.cpu()))","metadata":{"id":"fJe94ZFq762h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_image(image_path,checkpoint_model,save_path):\n    os.makedirs(os.path.join(save_path,\"results\"), exist_ok=True)\n\n    transform = test_transform()\n    transformer = TransformerNet().to(device)\n    transformer.load_state_dict(torch.load(checkpoint_model))\n    transformer.eval()\n\n    image_tensor = Variable(transform(Image.open(image_path))).to(device)\n    image_tensor = image_tensor.unsqueeze(0)\n\n    with torch.no_grad():\n        stylized_image = denormalize(transformer(image_tensor)).cpu()\n    \n    fn = checkpoint_model.split('/')[-1].split('.')[0]\n    save_image(stylized_image, os.path.join(save_path,f\"results/{fn}-output.jpg\"))\n    print(\"Image Saved!\")\n    plt.imshow(cv2.cvtColor(cv2.imread(os.path.join(save_path,f\"results/{fn}-output.jpg\")), cv2.COLOR_BGR2RGB))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Run this to test the model \"\"\"\ntest_check(test_dataset_path = '/content/dataset_new/test/test/',checkpoint_model = '/content/checkpoints/last_checkpoint.pth',image_size = 256)","metadata":{"id":"qawtdG2J742N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Run this to visualize the styled images \"\"\"\ntest_image(image_path = './content/japanese_garden.jpg',\n           checkpoint_model = '/content/checkpoints/last_checkpoint.pth',\n           save_path = './')","metadata":{},"execution_count":null,"outputs":[]}]}